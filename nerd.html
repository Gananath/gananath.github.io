<!DOCTYPE html>

<html lang="en">
<head>
<meta content="HTML Tidy for HTML5 for Linux version 5.2.0" name="generator"/>
<!-- Required meta tags -->
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<!-- Bootstrap CSS -->
<link href="./css/custom_bootstrap.css" rel="stylesheet"/>
<title>NERD: Evolution of Discrete data with Reinforcement Learning.</title>
<meta content="Evolution of Discrete data with Reinforcement Learning." name="description"/>
<style>
        .custom-btn {
            width: 17em !important;
        }
        .nav-link:visited,
            .nav-link:link {
              border-bottom: 2px solid transparent;
            }

        .nav-link:hover,
        .nav-link:active {
          border-bottom: 2px solid #1bcfc6;
        }
        </style>
</head>
<body>
<!-- Option 1: Bootstrap Bundle with Popper -->
<script src="./js/bootstrap.bundle.min.js">
</script>
<!-- Header -->
<div class="container px-2 border-bottom">
<nav class="navbar navbar-expand-lg navbar-light">
<a class="navbar-brand btn btn-outline-primary p-2 lead text-secondary" href="#" role="button">Gananath R</a>
<button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-bs-target="#navbarSupportedContent" data-bs-toggle="collapse" type="button">
<span class="navbar-toggler-icon"></span>
</button>
<div class="collapse navbar-collapse justify-content-end" id="navbarSupportedContent">
<ul class="navbar-nav ml-auto mb-2 mb-lg-0 lead px-2">
<li class="nav-item">
<a aria-current="page" class="nav-link" href="index.html">Home</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" data-bs-auto-close="outside" data-bs-toggle="dropdown" href="#" id="navbarDropdown" role="button">
                Projects
              </a>
<ul aria-labelledby="navbarDropdown" class="dropdown-menu">
<li class="dropstart">
<a class="dropdown-item dropdown-toggle" data-bs-toggle="dropdown" href="drugai.html">DrugAI
                </a>
<ul class="dropdown-menu">
<li><a class="dropdown-item" href="drugai.html">DrugAI</a></li>
<li><a class="dropdown-item" href="drugai-gen.html">DrugAI-gen</a></li>
<li><a class="dropdown-item" href="drugai-gan.html">DrugAI-GAN</a></li>
</ul>
</li>
<li></li>
<li></li>
<li></li>
<li></li>
<li><a class="dropdown-item" href="prob_retrieval.html">Probabilistic Memory Retrieval</a><a class="dropdown-item" href="proteinstatai.html">ProteinStatsAI</a><a class="dropdown-item" href="nerd.html">NERD</a><a class="dropdown-item" href="multi_task.html">BrainGAN</a><hr class="dropdown-divider"/></li>
<li class="dropstart">
<a class="dropdown-item dropdown-toggle" data-bs-toggle="dropdown" href="">Other
                </a>
<ul class="dropdown-menu">
<li><a class="dropdown-item" href="https://gist.github.com/Gananath/5b2f4640fdf96cd87a81a4206a0d3fe8">kaggle</a></li>
<li><a class="dropdown-item" href="real.html">Real</a></li>
<li><a class="dropdown-item" href="deepspectra.html">DeepSpectra</a></li>
<li><a class="dropdown-item" href="testing.html">Test</a></li>
</ul>
</li></ul>
</li>
<li class="nav-item">
<a class="nav-link" href="cv.html">CV</a>
</li>
<li class="nav-item dropdown">
<a aria-expanded="false" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" href="#" id="navbarDropdown" role="button">
                Contacts
              </a>
<ul aria-labelledby="navbarDropdown" class="dropdown-menu">
<li><a class="dropdown-item" href="#" onclick="return confirm('Msg me in github or reddit, preferably both.');">E-Mail</a></li>
<li><a class="dropdown-item" href="https://github.com/Gananath">GitHub</a></li>
<li><a class="dropdown-item" href="#">Facebook</a></li>
<li><a class="dropdown-item" href="#">Twitter</a></li>
</ul>
</li>
</ul>
</div>
</nav>
</div>
<!-- body -->
<div class="container">
<div class="row p-4 alert alert-success">
<h1 class="display-5">
                    NERD
</h1>
<hr/>
<h5 class="lead">
                    Evolution of Discrete data with Reinforcement Learning.
</h5>
</div>
<div class="row">
<!-- content -->
<div class="col-sm-9 border text-secondary rounded p-4"><p>
If you are visiting here for the first time
then I would recommend you to read my
earlier project <a href="drugai-gan.html">DrugAI</a> before going
any further. I am reusing many ideas and
functions that I have used in DrugAI
particularly from wGAN. In DrugAI project I
have used SMILES (Simplified Molecular
Input Line Entry System) for generating and
classifying drug like molecules. In simple
terms SMILES are ASCII strings representing
a struture of chemical molecule. For
example methane(CH4) in SMILES would be
written as "C" without quotes and ethane
(C2H6) as "CC".
</p>
<p>
NERD is the acronym for <kbd><i>Neural
Experimenting Rl algorithm for Discrete
data</i></kbd> (just wanted a cool name for
the project). NERD has ideas taken from
Genetic Algorithm and Reinforcement
Learning and combined it. You can think
NERD of as a chef, musician, scientist etc.
who experiments with his/her ingredients,
music notes or materials to create novel
cuisines or music or objects. In this
project the aim of NERD was to evolve
chemical molecules from scratch which are
in the SMILES format. I dont know if any
one else has already discovered such an
algorithm and hoping I am not reinventing
the wheel. The below image shows how NERD
algorithm works.
</p><img alt="NERD" class="img-fluid mx-auto d-block" src="images/nerd.jpg"/>
<h1 class="display-6 py-2">
NERD Algorithm
</h1>
<p>
NERD has two important parts, an agent
which is used for Reinforcement Learning
and Reward-Fitness Function (RFF) which
outputs reward and fitness for a given
SMILES molecule. Both of these parts have
neural network models at its core
</p>
<h1 class="display-6 py-2">
Reward-Fitness Funtion
</h1>
<p>
RFF at the center consists of a convolution
1D classification neural network. RFF's
classification model and the RL Agent have
similar neural network architecture except
the inputs and outputs. The output of RFF
consists of a discrete reward value as well
as a continous fitness scores. The reward
value is used for training the RL agent
whereas the fitness score is used for
selecting the best SMILES for next step.
The reward for a particlar SMILES struture
is determined by three things.
</p>
<ol>
<li>Whether classified SMILES is real or
fake.
</li>
<li>Whether the parent SMILES is same as
the child SMILES.
</li>
<li>Whether the generated sequence is in
SMILES format.
</li>
</ol>Fitness score on the other hand tells how
good the classification is. Fake classification
gets a negative fitness score. Fitness score
along with reward score is used for selecting
best candiates for next seed.
<p>
In my current project, RFF's neural network
is trained for 200 epochs. We have used
real and fake SMILES data to train this
neural network. The current accuracy of
RFF's model is near 50-60 ish
percentage.Not a good metrics but chose
this model for my lack of computational
power. We will use this model as a starting
function for creating reward and fitness
score.
</p>
<h1 class="display-6 py-2">
Reinforcement Learning Agent
</h1>
<p>
For NERD I have used a vanilla actor-critic
model as an RL agent. There is no other
reasons for choosing actor-critic model
except being easier to code in pytorch and
gives some what stable results. I have
followed the similar coding paradigm as
that of the official pytorch actor-critic
implementation. The main difference in
NERD's actor-critic model and official
pytorch's actor critic model is the inputs
and outputs. There are two inputs in NERD
one for the state and an another for z
vector. z vector is random numbers from
normal distribution. I tried training NERD
without z vector but the sequence generated
lack variety. The outputs for offical
pytorch implementaion have two heads a
policy head and a value head whereas NERD
have two policy head and a single value
head. The two policy heads in NERD's agent
have to make two separate decisions.
</p>
<ol>
<li>First Policy: What to do?
</li>
<li>Second Policy: Where to do it?
</li>
</ol>There are three things first policy head
can do. It can either mutate or delete a
character in the SMILES format or it can do
crossover. If you want to know more about
mutation, deletion and crossover then I will
recommend to refer some genetic books or read
about genetic algorithm (GA). Deletion is a
unique process in NERD when comparing to
vanilla GA. Sofar in my knowledge no one uses
deletion as a method to evolve something in
vanilla GA. The task of the second policy head
is much more simpler it has to deicide where it
has to do the mutation, deletion or crossover.
<h1 class="display-6 py-2">
RESULT
</h1>
<h5>
Sequence
</h5>
<p>
The RL agent training has been carried out
in google colab. The results are not
incredible but encouraging. When the
training started the output looked
something like this.
</p><kbd>Epoch: 0 Reward: -1000.0 Loss:
-4931.022</kbd>
<blockquote class="blockquote">
rC#1)+S6B4[))|||||||||||||||||||||||||||||||||...
-10.0 -0.821050<br/>
rC#1)+S6B4[))|||||||||||||||||||||||||||||||||...
-10.0 -0.900526<br/>
rC#1)+S6B4[))|||||||||||||||||||||||||||||||||...
-10.0 -0.956724<br/>
<br/>
<figcaption class="blockquote-footer">
    format = <i>[SMILES ,
    Reward,Fitness]</i>
</figcaption>
</blockquote>
<p>
Even after training for ten thousand epochs
the algorithm was not able to create a
valid SMILES but the end result looks
interesting. Even though its not SMILES
sequence, we can see its starting to learn
and understand it.
</p><kbd>Epoch: 10000 Reward: -1000.0 Loss:
-0.73</kbd>
<blockquote class="blockquote">
CCBCCCBCBCC|C||||CC|||||||||||||||||||||||||||...
-10.0 -0.501915<br/>
CCBCCCBCBCC|C||||CC|||||||||||||||||||||||||||...
-10.0 -0.502028<br/>
CCBCCCBCBCC|C||||CC|||||||||||||||||||||||||||...
-10.0 -0.502080<br/>
<br/>
</blockquote>
<p>
Ignoring the coding/implementaion error the
other most likely reasons the algorithm was
unsuccessful in generating SMILES sequence
is because it may require additonal
training time or of the subpar accuracy of
RFF model. Training a model to create a
sequence from scratch takes lots of
computation and thats why I think it is
much more suitable for refinig an already
existing sequence with this algorithm. This
way we can create new sequence from the pre
existing sequences.
</p>
<h1 class="display-6 py-2">
Images
</h1>
<p>
<b>Update: 2 DEC 2019</b><br/>
<img alt="nerd_mnist" class="img-fluid mx-auto d-block" src="https://raw.githubusercontent.com/Gananath/NERD/master/NERD_IMAGES/nerd_mnist.png"/>
</p>
<p>
Projects page <a href="https://github.com/Gananath/NERD">https://github.com/Gananath/NERD</a>
</p><br/>
</div><!-- Sidebar -->
<div class="col-sm-3 bg-light">
<a class="btn btn-primary btn-block custom-btn mt-1 text-light" href="index.html" role="button">Home</a> <a class="btn btn-primary btn-block custom-btn mt-1 text-light" href="drugai.html" role="button">DrugAI</a> <a class="btn btn-primary btn-block custom-btn mt-1 text-light" href="prob_retrieval.html" role="button">Probabilistic Memory Retrieval</a><a class="btn btn-primary btn-block custom-btn mt-1 text-light" href="proteinstatai.html" role="button">ProteinStatsAI</a><a class="btn btn-outline-primary text-secondary btn-block custom-btn mt-1" href="nerd.html" role="button">NERD</a><a class="btn btn-primary btn-block custom-btn mt-1 text-light" href="multi_task.html" role="button">BrainGAN</a><a class="btn btn-primary btn-block custom-btn mt-1 text-light" href="cv.html" role="button">CV</a> <a class="btn btn-primary btn-block custom-btn mt-1 text-light" href="https://github.com/Gananath" role="button">Contact</a>
</div>
</div>
</div><!-- Footer -->
<div class="container mt-4 border-top bg-primary rounded-bottom">
<footer class="py-3 my-4">
<ul class="nav justify-content-center border-bottom pb-3 mb-3">
<li class="nav-item">
<a class="nav-link px-2 text-light" href="#"></a>
</li>
</ul>
<p class="text-center text-light lead">
                    Â© 2022 Made by Me
                </p>
</footer>
</div>
</body>
</html>
